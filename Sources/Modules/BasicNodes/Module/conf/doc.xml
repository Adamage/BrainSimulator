<?xml version="1.0"?>
<doc>
    <assembly>
        <name>GoodAI.BasicNodes</name>
    </assembly>
    <members>
        <member name="T:BrainSimulator.Common.MyCsvFileWriterNode">
            <author>Josef Strunc</author>
            <status>Working</status>
            <summary>Node for generating Comma Separated Value files. </summary>
            <description>The generated file starts with (possibly several lines of) headers
            copied from the property "Headers". Then every other line contains consecutively:
            the time step, the label and input value (formatted according to the property
            "InputValueWriteFormat" separated by commas (no tabs or spaces).
            One line corresponds to one data point.
            </description>
        </member>
        <member name="T:BrainSimulator.CLA.MyNupicNode">
            <author>Josef Strunc</author>
            <status>Working</status>
            <summary>Node wrapping up the Nupic Core library (https://github.com/numenta/nupic.core)</summary>
            <description>Nupic is open source implementation of Numenta's cortical learning algorithm.
            This node uses three specific parts of the Nupic Core library (through the wrapper library NupicCoreSoloWrapper.dll):
            <ul>
            <li>SpatialPooler class,</li>
            <li>Cells4 class (temporeal pooler implementation) and</li>
            <li>FasfClaClassifier class</li>
            </ul>
            </description>
        </member>
        <member name="T:BrainSimulator.Testing.MyRandomNode">
            <author>Michal Vlasak</author>
            <status>Working</status>
            <summary>Generates numbers from chosen distribution</summary>
            <description></description>
        </member>
        <member name="T:BrainSimulator.Harm.MyAbstractDiscreteQLearningNode">
            <author>Jaroslav Vitku</author>
            <status>Under development</status>
            <summary>
            Parent of nodes that use discrete QLearning algorithms.
            </summary>
            <description>
            Parent of nodes that use discrete QLearning memory, which can be observed by the MyQMatrixObserver
            </description>
        </member>
        <member name="T:BrainSimulator.Harm.IDecisionSpace">
            <summary>
            Each stochastic return predictor has own DS, which contains subset of actions/vars
            of the Root Decision Space
            </summary>
        </member>
        <member name="M:BrainSimulator.Harm.IDecisionSpace.GetCurrentState">
            <summary>
            Read the current state while ignoring non-included variables
            </summary>
            <returns>list of indexes of maxVariables which contains only values of included variables</returns>
        </member>
        <member name="M:BrainSimulator.Harm.IDecisionSpace.GetLastExecutedActions">
            <summary>
            Return list of all actions that were just executed including abstract ones, not only those in the DS.
            </summary>
            <returns>List of aciton indexes</returns>
        </member>
        <member name="F:BrainSimulator.Harm.MyDecisionSpace.m_S_t">
            <summary>
            Holds vector of indexes of used variables. If the variable is removed, its last
            value should be left in the vector (so that the Q(s,a) matrix coords do not move too much).
            </summary>
        </member>
        <member name="M:BrainSimulator.Harm.MyDecisionSpace.GetLastExecutedAction">
            <summary>
            Determine one action that has been just executed. In case that multiple actions are found, 
            the one with the highest level is returned.
            </summary>
            <returns>Index of my action that has been executed, -1 if non of them.</returns>
        </member>
        <member name="M:BrainSimulator.Harm.MyDecisionSpace.AddAction(System.Int32)">
            <summary>
            Adds action, if the action is abstract, adds also its promoted variable to the DS.
            </summary>
            <param name="index">Index of action to be added.</param>
        </member>
        <member name="M:BrainSimulator.Harm.MyDecisionSpace.RemoveAction(System.Int32)">
            <summary>
            The same in the opposite direction; assumed: since the action, which controls this
            variable is not needed, also the variable is not needed.
            </summary>
            <param name="index">Index of action to be removed</param>
        </member>
        <member name="T:BrainSimulator.Harm.MyModuleParams">
            <summary>
            Global config class
            </summary>
        </member>
        <member name="T:BrainSimulator.Harm.MyEligibilityTrace">
            <summary>
            Remembers history of past N steps, learning algorithm computes delta 
            (one-step difference between expected and observed state value) and spreads this
            discounted value back into the history. This speeds up learning significantly.
            </summary>
        </member>
        <member name="M:BrainSimulator.Harm.MyDiscreteQLearning.Learn(System.Single,System.Int32[],System.Int32)">
            <summary>
            Performs Q-Learning (including the Eligibility Trace if enabled).
            </summary>
            <param name="r_t">Reward received as a result of previous action</param>
            <param name="s_tt">Current state s_t'</param>
            <param name="a_t">Previously executed action</param>
        </member>
        <member name="T:BrainSimulator.Harm.MyMotivationBasedDeleteUnselectedASM">
            <summary>
            Sets motivation values of non-selected acitons to the predefined value (e.g. 0).
            </summary>
        </member>
        <member name="T:BrainSimulator.Harm.MyMotivationSource">
            <summary>
            Produces motivation to execute the current strategy (abstract) - action.
            </summary>
        </member>
        <member name="M:BrainSimulator.Harm.MyMotivationSource.MakeStep">
            <summary>
            Different dynamics on different levels
            </summary>
        </member>
        <member name="T:BrainSimulator.Harm.IHierarchyMaintainer">
            <summary>
            Handles the correct: hierarchical decision-making, learning, hierarchy update
            </summary>
        </member>
        <member name="M:BrainSimulator.Harm.IHierarchyMaintainer.Learn">
            <summary>
            Each SRP check own reward, executes own learning.
            In the current verison the hierarchical depedencies are ignored (because they can be). 
            </summary>
        </member>
        <member name="M:BrainSimulator.Harm.IHierarchyMaintainer.InferUtilities">
            <summary>
            Infers utilities of primitive (all in fact) actions from the aciton hierarchy.
            The computed utilities are the final decision of the system.
            </summary>
        </member>
        <member name="M:BrainSimulator.Harm.IHierarchyMaintainer.Subspacing">
            <summary>
            Checks conditions for subspacing, subspaces based on data in the action/variable traces.
            </summary>
        </member>
        <member name="M:BrainSimulator.Harm.IHierarchyMaintainer.ManualSubspacing(System.Int32[],System.Int32[],System.Int32,System.String)">
            <summary>
            Create new abstract action - one StochasticReturnPredictor which promotes one variable.
            Level of the abstraction is detedmined by the actions contained in the action space.
            Note that the level of abstraction is defined by the child ACTIONS here!
            </summary>
            <param name="variables">list of variables to place in the DS</param>
            <param name="actions">list of actions to the DS</param>
            <param name="promotedVariable">variable that is promoted by this SRP</param>
            <param name="label">name of this "action"</param>
            <returns>newly created abstract action</returns>
        </member>
        <member name="M:BrainSimulator.Harm.IVariableHistory.AddVariable(System.Int32)">
            <summary>
            Add a variable to the history
            </summary>
            <param name="index">index of the variable in the RootDecisionSpace</param>
        </member>
        <member name="M:BrainSimulator.Harm.IVariableHistory.AddAllPotentialVariables">
            <summary>
            Remembers values of all added variables in time (or all possible variables discovered in the future)
            </summary>
        </member>
        <member name="T:BrainSimulator.Harm.MyLocalVariableHistory">
            <summary>
            The same as a VariableHistory, but each SRP has its own.
            </summary>
        </member>
        <member name="M:BrainSimulator.Harm.MyLocalVariableHistory.monitorVariableChanges(BrainSimulator.Harm.MyStochasticReturnPredictor)">
            <summary>
            This updates the variable weights in the DS, if the weight is under the threshold, 
            the corresponding variable should be removed from the DS.
            </summary>
            <param name="parent"></param>
        </member>
        <member name="M:BrainSimulator.Harm.MyLocalVariableHistory.performOnlineVariableRemoving(BrainSimulator.Harm.MyStochasticReturnPredictor)">
            <summary>
            This should be called only after receiving the reward. 
            Frequency of variable changes relative to rewards received.
            </summary>
            <param name="parent"></param>
        </member>
        <member name="M:BrainSimulator.Harm.MyVariableHistory.GetVariableChangedBefore(System.Int32)">
            <summary>
            Looks n steps into the history, checks for THE FIRST monitored variable that has changed.
            </summary>
            <param name="steps">number steps in the past</param>
            <returns>first identified variable that changed its value</returns>
        </member>
        <member name="M:BrainSimulator.Harm.MyVariableHistory.GetVariablesChangedBefore(System.Int32)">
            <summary>
            Return list of monitored variables that changed given no. of steps in the hsitory
            </summary>
            <param name="steps">number steps in the past (0 excluded)</param>
            <returns>list of all variable indexes that changed that time step</returns>
        </member>
        <member name="T:BrainSimulator.Harm.MyDiscreteHarmNode">
            <author>Jaroslav Vitku</author>
            <status>Working</status>
            <summary>
            Implements HARM based on the discrete Q-Learning algorithm.
            </summary>
            <description>
            HARM - Hierarchy, Abstraction, Rinforcements, Motivations. 
            A system, that is able to autonomously identify own capabilities and tries to learn them (in a SMDP environments).
            The system:
            
             <ul> 
               <li> Senses environment (vector of data: variables and constants) </li>
               <li> Acts in the environment (produces actions)</li>
               <li> Observes consequences of own actions. If some variable changes, the agent asumes that it is a consequence of its action and tries to learn this new ability:</li>
                <ul>
                <li> it creates new Stochastic Return Predictor (SRP) = decision space + discrete RL algorithm + source of motivation to execute this behavior</li>
                <li> this new SRP has a goal to learn how to change this variable. It is done by making connection link from variable change to reward of this SRP</li>
                <li> the decision space of this SRP should contain only subset of environment variables</li>
                <li> from now on, the SRP tries to learn how to change the variable by observing actions taken by the agent and receiving own reward</li>
                <li> all SRPs vote by publishing utilities of child actions in a given state.</li>
                <li> utilities produced by the SRP are scaled by the amoutn of motivation. Motivation increases in time and is set to 0 when the reward is received.</li>
               </ul>
             </ul> 
            
            The node publishes utilities of primitive actions in a given state. 
            The utility of action is computed as a sum of utilities (scaled by motivations) from all parent SRPs. 
            Therefore all SRPs learn in parallel and resulting strategy followed by the agent is a result of all intentions of the agent.
            <br>
            Current version uses one level of hierarchy of RL decision spaces based on interaction 
            with the environment.
            </br>
            <h3>Before use:</h3>
             <ul> 
                <li> Works only in environments which fulfill the SMDP (Semi-Markov Decision Process) environments.</li>
                <li> Works only for positive integer values of variables. Need to rescale input values by the RescaleVariables parameter. </li>
                <li> Currently uses one level of hierarchy of SRPs.</li>
                <li> Variable and action subspacing should be used carefully with respect to the environment, or disabled.</li>
             </ul> 
            </description>
        </member>
        <member name="T:BrainSimulator.Harm.MyDiscreteHarmNode.MyVariableUpdateTask">
            <summary>
            Monitors which variables should be contained in the decision space.
            
            For each dimension of GlobalDataInput checks if there were multiple values, 
            if yes, it is identified as a variable and added into the DS.
            
            Also, inputs are rescaled by the value of RescaleVariables.
            </summary>
        </member>
        <member name="T:BrainSimulator.Harm.MyDiscreteHarmNode.MyActionUpdateTask">
            <summary>
            Number of primitive actions is predefined, abastract acitons are created here.
            </summary>
        </member>
        <member name="T:BrainSimulator.Harm.MyDiscreteHarmNode.MyHierarchicalLearningTask">
             <summary>
             Implements Q-learning in the action hiearchy
            
             <h3>Parameters</h3>
             <ul>
                 <li><b>Alpha: </b>Learning factor - the higher the faster the learning is.</li>
                 <li><b>Gamma: </b>How far into the future algorithm looks for learning.</li>
                 <li><b>RewardScale: </b>Increases the value of reward stored (helps increasing accuracy).</li>
                 <li><b>Lambda: </b>How strongly are past values updated (bigger values increase the learning speed, but can cause learning oscilations).</li>
                 <li><b>EligibilityTraceEnabled: </b>Use the trace?</li>
                 <li><b>EligibilityTraceLen: </b>How many past steps to update at once (determined efectively by the lambda parameter).</li>
             </ul>
             </summary>
        </member>
        <member name="T:BrainSimulator.Harm.MyDiscreteHarmNode.MyHierarchyCreationTask">
            <summary>
            Implements subspacing (on a global level), variable/action removing/addding.
            
            <h3>Parameters</h3>
            <ul>
                <li><b>ActionTraceLength: </b>Length of trace meory for actinos.</li>
                <li><b>VariableTraceLength: </b>Length of trace meory for variables.</li>
                <li><b>ActionSubspacingThreshold: </b>The lower the threshold, the less actions will be in the Decision Space.</li>
                <li><b>VariableSubspacingThreshold: </b>The lower the threshold, the less variables will be in the Decision Space.</li>
                <li><b>HistoryForgettingRate: </b>How to weight the importance of actions/variables in the past.</li>
                <li><b>SubspaceActions: </b>Subspace actions, or use all actions in new Decision Space.</li>
                <li><b>SubspaceVariables: </b>Subspace variables? If disabled, all sensory data are considered in all decision spaces.</li>
                <li><b>OnlineSubspaceVariables: </b>Subspace variables online? If some variable does not change often enough (relatively to receving reward) can be removed from the DS..</li>
                <li><b>OnlineHistoryForgettingRate: </b>If variable changes 1 is added to its value. All values are decayed each step by the OnlineHistoryForgettingRate.</li>
                <li><b>OnlineVariableRemovingThreshold: </b>After receiving the reward, the all variables in the DS are checked how often they changed in the history. If the value is under this threshold, variable will be removed from the DS.</li>
            </ul>
            </summary>
        </member>
        <member name="T:BrainSimulator.Harm.MyDiscreteHarmNode.MyHierarchicalActionSelection">
            <summary>
            Infers utility values of primitive actions from the RL hierarchy. Publishes utilities.
            
            <h3>Parameters</h3>
            <ul>
                <li><b>MotivationChange: </b>How much to increase the motivation each step if no reward is received.</li>
                <li><b>UseHierarchicalASM: </b>Eeach SRP publishes utilities of all actions or should use own Action Selection Method (ASM) (select only one action)?.</li>
                <li><b>MinEpsilon: </b>If the UseHierarchicalASM is enabled, this is minimum probability of randomization in the ASM.</li>
                <li><b>PropagateUtilitiesInHierarchy: </b>If disabled, utility does not propagete from SRPs to child actions.</li>
            </ul>
            </summary>
        </member>
        <member name="M:BrainSimulator.Harm.MyDiscreteHarmNode.MyHierarchicalActionSelection.Execute">
            <summary>
            Infer utility values of primitive actions from the hierarchy and publish.
            </summary>
        </member>
        <member name="T:BrainSimulator.Harm.MyDiscreteHarmNode.MyPostSimulationStepTask">
            <summary>
            Mandatory task, cleans-up data after each step.
            </summary>
        </member>
        <member name="T:BrainSimulator.Harm.MyDiscreteHarmNode.MyMotivationOverrideTask">
            <summary>
            Monitors manual motivation inputs. 
            Overrides all inner motivations for actions if the threshold on the input MotivationsOverride is above 0.5.
            </summary>
        </member>
        <member name="T:BrainSimulator.Harm.MyDiscreteHarmNode.MyActionUtilsVisualization">
            <summary>
            Only a placeholder for Observer methods and it does need to be enabled.
            </summary>
        </member>
        <member name="M:BrainSimulator.Harm.MyDiscreteHarmNode.MyActionUtilsVisualization.GetSRPForVar(System.Int32)">
            <summary>
            If the variable can be controlled by own SRP, return it.
            </summary>
            <param name="ind">Global index of variable</param>
            <returns>SRP that controls the state of this variable</returns>
        </member>
        <member name="M:BrainSimulator.Harm.MyDiscreteHarmNode.MyActionUtilsVisualization.ReadTwoDimensions(System.Single[0:,0:]@,System.Int32[0:,0:]@,BrainSimulator.Harm.MyStochasticReturnPredictor,System.Int32,System.Int32,System.Boolean)">
            <summary>
            For a given predictor, the method creates 2D array of max action utilities and max action labels over selected dimensions.
            The values in the memory are automatically scaled into the interval 0,1. Realtime values are multililed by motivations (therfore are bigger).
            </summary>
            <param name="values">array passed by reference for storing utilities of best action</param>
            <param name="labelIndexes">array of the same size for best action indexes</param>
            <param name="predictor">an asbtract action</param>
            <param name="XVarIndex">global index of state variable in the VariableManager</param>
            <param name="YVarIndex">the same: y axis</param>
            <param name="showRealtimeUtilities">show current utilities (scaled by motivations from the source and the hierarchy?)</param>
        </member>
        <member name="M:BrainSimulator.Harm.MyDiscreteHarmNode.MyActionUtilsVisualization.ReadSize(BrainSimulator.Harm.IDecisionSpace,BrainSimulator.Harm.MyVariable,System.Int32,System.String)">
            <summary>
            Size is given by the no of variables , but only in case that the variable is contained in the ds.  
            </summary>
            <returns></returns>
        </member>
        <member name="T:BrainSimulator.Observers.MyAbstractQLearningObserver`1">
            <summary>
            Observes valdata stored in the QMatrix.
            </summary>
            <typeparam name="T">Node which uses DiscreteQLearnin to be observed</typeparam>
        </member>
        <member name="M:BrainSimulator.Observers.MyAbstractQLearningObserver`1.MatrixSizeOK">
            <summary>
            Is not OK either if both dimensions have zero size or the total size of matrix is too big for GPU mem
            </summary>
            <returns></returns>
        </member>
        <member name="T:BrainSimulator.Observers.MyQLearningObserver">
            <summary>
            Observes only flat QLearning node.
            </summary>
        </member>
        <member name="T:BrainSimulator.Observers.MySRPObserver">
            <summary>
            Observers SRPs (Stochastic Return Predictors) in the HARM (each one has name, own motivation, promoted variable etc..).
            </summary>
        </member>
        <member name="T:BrainSimulator.Harm.MyRootDecisionSpace">
            <summary>
            Holds all known actions and variables.
            </summary>
        </member>
        <member name="M:BrainSimulator.Harm.MyAction.AddPromotedVariable(System.Int32,BrainSimulator.Harm.MyRootDecisionSpace)">
            <summary>
            Works with only one variable
            </summary>
            <param name="varIndex"></param>
        </member>
        <member name="M:BrainSimulator.Harm.MyAction.SetJustExecuted(System.Boolean)">
            <summary>
            Primitive actions are updated on the bottom, abstract aciton sets this by itself.
            </summary>
            <param name="yes">Whether the action (or child action) has just been executed.</param>
        </member>
        <member name="M:BrainSimulator.Harm.MyAction.JustExecuted">
            <summary>
            This indicates that the action has been executed in the current step. 
            It is used in the higher levels of hierarchy: higher level action "hides" execution
            of lower level actions that are part of its decision space (policy). 
            </summary>
            <returns>True if the action (or some acton from my decuison sapce) has jsut been executed.</returns>
        </member>
        <member name="M:BrainSimulator.Harm.MyAction.GetMyTotalMotivation">
            <summary>
            TotalUtility is composed of sum of utilities promoted from parents and my Motivation (0 here).
            </summary>
            <returns>Total utility of the action (motivation*RS + utils.from parents)</returns>
        </member>
        <member name="T:BrainSimulator.Harm.MyMotivatedAction">
            <summary>
            Action, that has own source of motivation with own inner dynamics.
            </summary>
        </member>
        <member name="T:BrainSimulator.Harm.ActionManager">
            <summary>
            Stores all aciton that the agent is capable of (primitive and abstract ones).
            New actions are added online, the old ones are disabled in the particular spaces.
            </summary>
        </member>
        <member name="T:BrainSimulator.Harm.VariableManager">
            <summary>
            Stores all variables that agent have seen so far (list is updated from the
            input data).
            </summary>
        </member>
        <member name="M:BrainSimulator.Harm.VariableManager.GetCurrentState">
            <summary>
            Decode the current state based on all known variables and their current values.
            </summary>
            <returns>List of indexes (only states) to be used in the Q(s,a) memory.</returns>
        </member>
        <member name="T:BrainSimulator.Harm.IStochasticReturnPredictor">
            <summary>
            SRP is a standalone system, which includes: decision space, learning algorithm, action selection.
            </summary>
        </member>
        <member name="M:BrainSimulator.Harm.IStochasticReturnPredictor.Learn">
            <summary>
            Update the Q(s,a) memory
            </summary>
        </member>
        <member name="M:BrainSimulator.Harm.IStochasticReturnPredictor.SelectAction">
            <summary>
            Compute Utilities, Select action (changes utility values), scale utilities, propagate them to childs.
            </summary>
        </member>
        <member name="M:BrainSimulator.Harm.IStochasticReturnPredictor.PostSimulationStep">
            <summary>
            Should be called after the simulation step.
            </summary>
        </member>
        <member name="M:BrainSimulator.Harm.IStochasticReturnPredictor.GetMaxMemoryValue">
            <summary>
            for scaling memory values to the interval 0,1
            </summary>
            <returns></returns>
        </member>
        <member name="M:BrainSimulator.Harm.IStochasticReturnPredictor.AddNewVariable(System.Int32)">
            <summary>
            If new variable is detected in the DS, it should be added into all decision spaces,
            knowledge then can be shared from the old memory. 
            
            Then the strategy is updated for nev values by learning. 
            If both strategies (old and new one) are "the same", the variable can be then removed.
            </summary>
            <param name="no">index of new variale</param>
        </member>
        <member name="T:BrainSimulator.Harm.MyStochasticReturnPredictor">
            <summary>
            Is an abstract action, which lears in own decision space to get own type of reward.
            </summary>
        </member>
        <member name="M:BrainSimulator.Harm.MyStochasticReturnPredictor.PerformActionAdding(System.Collections.Generic.List{System.Int32})">
            <summary>
            This adds all primitive actions, that were just executed. 
            Note that multilevel hierarchy is built by variable adding in the current version.
            </summary>
            <param name="actionsJustExecuted">lsit of actions that were just executed</param>
        </member>
        <member name="T:BrainSimulator.LSTM.MyLSTMLayer">
            <author>Karol Kuna</author>
            <status>WIP</status>
            <summary>Long Short Term Memory layer</summary>
            <description></description>
        </member>
        <member name="T:BrainSimulator.NeuralNetwork.Group.MyNeuralNetworkGroup">
            <author>Philip Hilm</author>
            <status>WIP</status>
            <summary>Network node group.</summary>
            <description></description>
        </member>
        <member name="T:BrainSimulator.NeuralNetwork.Group.MyQLearningGroup">
            <author>Philip Hilm</author>
            <status>WIP</status>
            <summary>QLearning group.</summary>
            <description>
                Group with the functionality and execution planning needed for QLearning.
                Derived from NeuralNetworkGroup
            </description>
        </member>
        <member name="T:CustomModels.NeuralNetwork.Layers.MyActivationLayer">
            <author>Mikulas Zelinka</author>
            <status>WIP</status>
            <summary>Activation layer.</summary>
            <description>Activation layer of the NN group.</description>
        </member>
        <member name="T:BrainSimulator.NeuralNetwork.Layers.MyConvolutionLayer">
            <author>Mikulas Zelinka</author>
            <status>WIP</status>
            <summary>Convolutional layer.</summary>
            <description></description>
        </member>
        <member name="T:BrainSimulator.NeuralNetwork.Layers.MyHiddenLayer">
            <author>Philip Hilm</author>
            <status>WIP</status>
            <summary>Hidden layer node.</summary>
            <description></description>
        </member>
        <member name="T:BrainSimulator.NeuralNetwork.Layers.MyLayer">
            <author>Philip Hilm</author>
            <status>OBSOLETE</status>
            <summary>Neural layer node.</summary>
            <description></description>
            
            
            WARNING: THIS LAYER IS NOW OBSOLETE. USE MYHIDDENLAYER / MYOUTPUTLAYER
        </member>
        <member name="T:BrainSimulator.NeuralNetwork.Layers.MyOutputLayer">
            <author>Philip Hilm</author>
            <status>WIP</status>
            <summary>Output layer node.</summary>
            <description></description>
        </member>
        <member name="T:BrainSimulator.NeuralNetwork.Layers.MyPoolingLayer">
            <author>Mikulas Zelinka</author>
            <status>WIP</status>
            <summary>Pooling layer.</summary>
            <description></description>
        </member>
        <member name="T:BrainSimulator.RBM.MyRBMGroup">
            <author>Mikulas Zelinka</author>
            <status>Working</status>
            <summary>
                Node group used for Restricted Boltzmann Machines and deep learning.
                Derived from Neural Network group whose functionality it inherits.
            </summary>
            <description>
                Node group used for Restricted Boltzmann Machines and deep learning.
                Derived from Neural Network group whose functionality it inherits.
            </description>
        </member>
        <member name="T:BrainSimulator.RBM.MyRBMInputLayer">
            <author>Mikulas Zelinka</author>
            <status>Working</status>
            <summary>
                Input layer of Restricted Boltzmann Machine network.
                Can only act as a visible layer. In an RBM network, there is only one input layer and it is its first node.
            </summary>
        </member>
        <member name="M:BrainSimulator.RBM.MyRBMInputLayer.RBMRandomActivation">
            <summary>
            Randomly activates the output neurons.
            </summary>
        </member>
        <member name="T:BrainSimulator.RBM.MyRBMLayer">
            <author>Mikulas Zelinka</author>
            <status>Working</status>
            <summary>
                One layer of Restricted Boltzmann Machine network.
                Can act as both visible and hidden.
            </summary>
        </member>
        <member name="M:BrainSimulator.RBM.MyRBMLayer.RBMRandomActivation">
            <summary>
            Randomly activates the output neurons.
            </summary>
        </member>
        <member name="T:BrainSimulator.SparseCoding.MyScalarToSDRNode">
            <author>Josef Strunc</author>
            <status>WIP</status>
            <summary>Encodes scalar values into Sparse Distributed Representation
            with the required properties for the CLA Node</summary>
            <description>n-tuple of scalar values is encoded into [n x LENGTH] binary matrix, where each
            row corresponds to its respective scalar value.
            The input scalar values have to be inside the interval [MIN, MAX], otherwise they are cropped
            to this interval (lower value than MIN is set to MIN, larger value than MAX is set to MAX).
            Parameters:<br />
            <ul>
            <li>MIN: minimal value of 1 input scalar value in the input vector</li>
            <li>MAX: maximal value of 1 input scalar value in the input vector</li>
            <li>LENGTH: length of the binary vector that encodes one scalar value</li>
            <li>ON_BITS_LENGTH: the number of bits equal to 1; should be around 2% of the LENGTH value</li>
            <li>RESOLUTION: read-only parameter - the smallest quantization step of input value that is preserved by the encoding</li>
            </ul>
            </description>
        </member>
        <member name="T:BrainSimulator.Clustering.MyKMeansNode">
            <author>Radoslav Bielek</author>
            <status>WIP</status>
            <summary>Not finished version of the K-means algorithm</summary>
            <description></description>
        </member>
        <member name="T:BrainSimulator.Common.MyGenerateInput">
            <author>Martin Balek</author>
            <status>Working</status>
            <summary>Samples a linear function values to the output array. 
            The output is shifted each step by ShiftSpeed parameter.
            </summary>
            <description></description>
        </member>
        <member name="T:BrainSimulator.Nodes.MySignalNode">
            <author>Dusan Fedorcak</author>
            <status>Working</status>
            <summary>Alters signal on the data connection according to signal input.</summary>
            <description>Select a signal name and threshold to raise/drop selected signal on the data output.</description>
        </member>
        <member name="T:BrainSimulator.GameBoy.My2DAgentWorld">
            <author>Dusan Fedorcak</author>
            <status>WIP</status>
            <summary>Simple 2D topview world with agent and target.</summary>
            <description>Agent and target positions are available. Agent movement can be controlled.</description>
        </member>
        <member name="T:BrainSimulator.GameBoy.MyCustomPongWorld">
            <author>Dusan Fedorcak</author>
            <status>Working</status>
            <summary>Custom implementation of the pong (arkanoid) game.</summary>
            <description>Follows the original game boy EVA pong game (same graphics &amp; levels). Bricks can be turned on and off. Ball can be fired in arbitrary direction. It is faster.</description>
        </member>
        <member name="T:BrainSimulator.GameBoy.MyCustomPongWorld.MyUpdateTask">
            <summary>
            BinaryEvent output has vector of binary events as follows: "bounce ball", "brick destroyed", "lost life".
            </summary>
        </member>
        <member name="T:BrainSimulator.GridWorld.IWorldEngine">
            <summary>
            Defines the behavior of all objects in the world.
            </summary>
        </member>
        <member name="M:BrainSimulator.GridWorld.SimpleGridWorldEngine.PrintNames(System.Boolean)">
            <summary>
            Prints names of output data.
            </summary>
            <param name="useConstants">Print names of global Data, or only variables?</param>
            <returns>String containing all names separated by spaces</returns>
        </member>
        <member name="M:BrainSimulator.GridWorld.SimpleGridWorldEngine.GetGlobalOutputVarNames">
            <summary>
            Return vector of variable names (corresponds to the GlobalVariable data output)
            </summary>
            <returns></returns>
        </member>
        <member name="M:BrainSimulator.GridWorld.SimpleGridWorldEngine.GetGlobalOutputDataNames">
            <summary>
            Return vector of all data names (corresponds to the GlobalData data output), includes also all constants
            </summary>
            <returns></returns>
        </member>
        <member name="T:BrainSimulator.GridWorld.MyMapH">
            <summary>
            definitions of custom maps here
            </summary>
            
        </member>
        <member name="T:BrainSimulator.GridWorld.Tale">
            <summary>
            end of custom map definitions
            </summary>
        </member>
        <member name="T:BrainSimulator.GridWorld.MyGridWorld">
            <author>Jaroslav Vitku</author>
            <status>Working</status>
            <summary>A simple discrete simulator with maze and controllable objects.</summary>
            <description>
            
            A discrete world with one agent, the agent can:
            <ul>
                <li>Do nothiong (action 0)</li>
                <li>Move in 4 directions (actions 1,2,3,4)</li>
                <li>and control controllable objects (action 5).</li>
            </ul>
            
            The world is composed of:
            <ul>
                <li>an agent</li>
                <li>free tales</li>
                <li>obstacles</li>
                <li>two types of static objects: controllable (door switch, lights switch) and controlled objects (door, lights)</li>
            </ul>
            
            
            
            <h3>Outputs</h3>
            <ul>
                <li> <b>Global:</b> publishes all information about the world, that is:
            agent's position and (changeable) properties of all objects with their positions.</li>
                <li> <b>Variables:</b> publishes only all variables in the world (typically omits positions
            of static objects), for testing purposes.</li>
            <li> <b>AgentPosX:</b> Agent's current position on the X axis.</li>
            <li> <b>AgentPosY:</b> Agent's current position on the Y axis.</li>
            <li> <b>Visual:</b> bitmap representing the current world state.</li>
            <li> <b>EgocentricVisual:</b> egocentric view of the agent with predefined size</li>
            <li> <b>MapSizeOutput:</b> publishes the following vector of information about wolrd size: [maxX, maxY, 1/maxX, 1/maxY].</li>
            </ul>
            
            
            <h3>Inputs</h3>
            <ul>
                <li> <b>Action:</b> Vector indicating the selected action. The index of maximum value is evaluated as a selected action.</li>
            </ul>
            
            <h3>Parameters</h3>
            <ul>
                <li><b>ShowInEgocentricView: </b>show the agent in egocentric view?</li>
                <li><b>EgocentricViewLimit: </b>size of the egocentric visual</li>
                <li><b>WorldBoundsValue: </b>used for boundaries in the egocentric view</li>
                <li><b>USED_MAP: </b>chooses one of predefined maps</li>
                <li><b>TEXTURE: </b>different textures for the visual representation</li>
            </ul>
            
            </description>
        </member>
        <member name="T:BrainSimulator.GridWorld.MyGridWorld.MyInitTask">
            <summary>
            Initialize the world (load graphics etc.).
            </summary>
        </member>
        <member name="T:BrainSimulator.GridWorld.MyGridWorld.MyRenderTask">
            <summary>
            Renders the visible area, not needed for simulation.
            </summary>
        </member>
        <member name="T:BrainSimulator.GridWorld.MyGridWorld.MyUpdateTask">
            <summary>
            Update the world state based on actions, read the new state and publish it.
            
            <description>
            <h3>Parameters</h3>
            <ul>
                <li><b>ForceDoorSwitches: </b>forces the state of door switches in a selected position / agent cannot change it</li>
                <li><b>ForceDoorSwitchesState: </b>defines state of all door switches in the game, if ForceDoorSwitches is enabled</li>
                <li><b>ForceLightSwitches: </b>forces the state of light switches in a selected position / agent cannot change it</li>
                <li><b>ForceLightSwitchesState: </b>defines state of all light switches in the game, if ForceLightSwitches is enabled</li>
                <li><b>LimitFieldOfView: </b>if enabled, only those values of World's output are updated, which correspond to objects that are in the agent's visible area</li>
                <li><b>DoorsAlwaysPassable: </b>if enabled , the agent is always able to go through the door</li>
                <li><b>ContinuousMovement: </b>if enabled , the agent moves in a continuous way</li>
            </ul>
            </description>
            </summary>
        </member>
        <member name="T:BrainSimulator.Harm.IDimList">
            <summary>
            Implements multidimensional matrix as recursive list of lists of depth = no. dimensions.
            </summary>
        </member>
        <member name="T:BrainSimulator.Harm.MyDetectChangesNode">
            <author>Jaroslav Vitku</author>
            <status>Working</status>
            <summary>
            Computes direction of change of the input variables (independently). 
            </summary>
            <description>
            The value on the output is given by parameter OutputScale and is zero/positive/negative. Used for producing rewards to the DiscreteQLearningNode.
            </description>
        </member>
        <member name="T:BrainSimulator.Harm.MyDetectChangesNode.MyDifferenceDetector">
            <summary>
            Detect difference compared to previous input and publish it.
            </summary>
        </member>
        <member name="T:BrainSimulator.Harm.MyDiscreteQLearningNode">
             <author>Jaroslav Vitku</author>
             <status>Working</status>
             <summary>
             Implements Q-Learning (Q-Lambda) algorithm based on matrix representation of data.
             </summary>
             <description>
             Node does the following:
                <ul>
                <li> <b>Reveives data: </b>Accepts world state description and currently selected action (code 1ofN).</li>
                <li> <b>Learns: </b>Updates own Q(s,a) matix.</li>
                <li> <b>Publishes what it has learned: </b>Publishes utilities of actions in the current state, these are scaled by the value of motivation.</li>
               </ul>
            
             Note that input data have to be rescaled to integer values.  
             No. of matrix dimensions and dimension sizes adapts itself to the size of data seen so far 
             (that is no. of variables and no. of values of each variable).
             
             <h3>Inputs</h3>
             <ul>
                 <li> <b>GlobalData:</b> Vector describing state of the environment (can contain variables and constants). Values are scaled by the InputRescaleSize parameter.</li>
                 <li> <b>SelectedAction:</b> Vector of size [numberOfActions]. The highest value indicates action that has been executed by the agent.</li>
                 <li> <b>Reward:</b>If the value is non-zero, the RL receives the reward.</li>
                 <li> <b>Motivation:</b>Scales action utilities: the higher the motivation, higher values on the output.</li>
             </ul>
             <h3>Output</h3>
             <ul>
                 <li> <b>Utilities:</b> Vector of action utilities in a given state (the higher the value, the better to use the action).</li>
             </ul>
             <h3>Parameters</h3>
             <ul>
                 <li> <b>InputRescaleSize:</b>Memory is indexed by integers, user should ideally rescale variable values to fit into integers.</li>
                 <li> <b>Number of Primitive Actions:</b>Number of primitive actions produced by the agent (e.g. 6 for the current gridworld, 3 for the breakout game).</li>
                 <li> <b>SumRewards:</b>Sum across the values in the vector of rewards?</li>
             </ul>
             
             </description>
        </member>
        <member name="T:BrainSimulator.Harm.MyDiscreteQLearningNode.MyVariableUpdateTask">
            <summary>
             <ul>
               <li>  Checks which variables should be contained in the decision space.</li>
               <li>  For each dimension of GlobalDataInput checks if there were multiple values, 
            if yes, it is identified as a variable and added into the DS. </li>
               <li>  Also, inputs are rescaled by the value of RescaleVariables.</li>
              </ul>
            </summary>
        </member>
        <member name="T:BrainSimulator.Harm.MyDiscreteQLearningNode.MyLearnTask">
            <summary>
            Updates the values of the Q(s,a) matrix by means of Q-Learning with eligibility trace.
            
            <h3>Parameters</h3>
            <ul>
                <li><b>Alpha: </b>Learning factor - the higher the faster the learning is.</li>
                <li><b>Gamma: </b>How far into the future algorithm looks for learning.</li>
                <li><b>RewardScale: </b>Increases the value of reward stored (helps increasing accuracy).</li>
                <li><b>Lambda: </b>How strongly are past values updated (bigger values increase the learning speed, but can cause learning oscilations).</li>
                <li><b>EligibilityTraceEnabled: </b>"Use the thrace?</li>
                <li><b>EligibilityTraceLen: </b>How many past steps to update at once (determined efectively by the lambda parameter).</li>
            </ul>
            
            </summary>
        </member>
        <member name="T:BrainSimulator.Harm.MyDiscreteQLearningNode.MyReadUtilsTask">
            <summary>
            Reads utility values in the current state, rescales them by the MotivationInput and publishes to the UtilityOutput.
            </summary>
        </member>
        <member name="T:BrainSimulator.Harm.MyDiscreteQLearningNode.MyActionUtilsVisualization">
            <summary>
            Updates the <b>RewardStats</b> values = {Reward/step, totalReward}.
            </summary>
        </member>
        <member name="M:BrainSimulator.Harm.MyDiscreteQLearningNode.MyActionUtilsVisualization.ReadTwoDimensions(System.Single[0:,0:]@,System.Int32[0:,0:]@,System.Int32,System.Int32,System.Boolean)">
            <summary>
            Method creates 2D array of max action utilities and max action labels over across selected dimensions.
            The values in the memory are automatically scaled into the interval 0,1. Realtime values are multiplied by motivations.
            </summary>
            <param name="values">array passed by reference for storing utilities of best action</param>
            <param name="labelIndexes">array of the same size for best action indexes</param>
            <param name="XVarIndex">global index of state variable in the VariableManager</param>
            <param name="YVarIndex">the same: y axis</param>
            <param name="showRealtimeUtilities">show current utilities (scaled by the current motivation)</param>
        </member>
        <member name="T:BrainSimulator.Harm.MyActionSelectionNode">
            <author>Jaroslav Vitku</author>
            <status>Working</status>
            <summary>Implements motivation-based action selection method. Motivation weights between greedy and random strategy.</summary>
            <description>
            
            <ul>
                <li> Uses epsilon-greedy action seleciton for selecting action based on utility values on inputs.</li>
                <li> Epsilon defines amount of randomization in the greedy strategy and is altered by the motivation input: the higher motivation -> the less randomization (more greedy strategy).</li>
                <li> Selected action is published in 1ofN code.</li>
            </ul>
            
            <h3>Inputs</h3>
            <ul>
            <li><b>Utilities: </b>Vector of utility values corresponding to particular actions (e.g. produced by the DiscreteQLearningNode).</li>
            <li><b>Motivation: </b>Current amount of motivation to use the learned strategy (weights exploitation vs. exploration of the strategy).</li>
            </ul>
            <h3>Outputs</h3>
            <ul>
            <li><b>SelectedAction: </b>Action that was selected, coded in 1ofN code (e.g. to be received by the DiscreteQLearningNode(s)).</li>
            </ul>
            </description>
        </member>
        <member name="T:BrainSimulator.Harm.MySimpleSortTask">
            <summary>
            Choose the best action according to the probability of M, otherwise completely random.
            
            If there are multiple identical highest values, choose randomly from all.
            
            <h3>Parameters</h3>
            <ul>
            <li><b>Selection Period: </b>Select new action each N steps</li>
            <li><b>Min Epsilon: </b>Minimum probability of randomization (in case that Motivation=1)</li>
            </ul>
            </summary>
        </member>
        <member name="T:BrainSimulator.Harm.IDiscreteQSAMemory">
            <summary>
            Defines the implementation of memory with adaptable dimensions and dimension sizes. 
            </summary>
        </member>
        <member name="M:BrainSimulator.Harm.IDiscreteQSAMemory.WriteData(System.Int32[],System.Int32,System.Single)">
            <summary>
            Place the utility of action into the memory, memory allocated if needed.
            </summary>
            <param name="stateValues">Array of indexes definind the current state</param>
            <param name="action">Current action to be written in the state</param>
            <param name="value">Utility value of the action</param>
        </member>
        <member name="M:BrainSimulator.Harm.IDiscreteQSAMemory.DisableDimension(System.Int32)">
            <summary>
            Set the dim. list sizes to 0 in the simplest case. This indicates that the dimension is not used.
            
            Possible improvement1: recursivelly deallocate the memory
            Possible improvement2: average all utilities accross the deleted dimension (knowledge reuse).
            </summary>
            <param name="no">index of dimension to be deleted</param>
        </member>
        <member name="M:BrainSimulator.Harm.IDiscreteQSAMemory.ReadData(System.Int32[])">
            <summary>
            Read utility values in a given state. 
            If there is no data for action, the default values are returned.
            </summary>
            <param name="states">Array of indexes describing the curren state</param>
            <returns>Array of aciton utilities, for each of currently used (and non-disabled) actions one value</returns>
        </member>
        <member name="M:BrainSimulator.Harm.IDiscreteQSAMemory.GetDimensionSizes">
            <summary>
            Get list of sizes of particular dimensions (the last one is (max) no. of actions)
            </summary>
            <returns>List of dimension sizes</returns>
        </member>
        <member name="M:BrainSimulator.Harm.IDiscreteQSAMemory.GetMaxStateVariables">
            <summary>
            Array of indexes used for indexing the matrix is finite -> also the max no. of state variables.
            </summary>
            <returns>Returns max number of state variables</returns>
        </member>
        <member name="T:BrainSimulator.Matrix.MyMatrixCPUOps">
            <summary>
            
            </summary>
        </member>
        <member name="T:BrainSimulator.Matrix.MyMatrixOps">
            <summary>
            Strategy DesignPatern:
               This is the abstract class that defines what will happen, then specific instance (that depends on the execuion=operation type (CPU/GPU/cublas..)) will execute the queried operation
            </summary>
        </member>
        <member name="T:BrainSimulator.Matrix.MyDataDistNode">
            <author> Honza Knopp</author>
            <status> Under dev., only a few of basic functions so far...</status>
               
            <summary>
            </summary>
            <description>
            <h2> TODO: </h2>
               User input (i.e. A+B*3-5)
            </description>
        </member>
        <member name="T:BrainSimulator.Matrix.MyMatrixAutoOps">
             <summary>
             Class for performing matrix operations
             </summary>
             
             <h2> Usage </h2>
               MyMatrixAutoOps mat_operation;
             
               //--- in init task
               mat_operation = new MyMatrixAutoOps(Owner, Matrix.MatOperation.Multiplication | Matrix.MatOperation.Addition, A); // It may need A for setting up kernel size!  Operation is for 
            
             
              //--- when you need to use it
              mat_operation.Run(Matrix.MatOperation.Multiplication, A, B, Output1);
              mat_operation.Run(Matrix.MatOperation.Multiplication, A, 10.4f, Output2);
              mat_operation.Run(Matrix.MatOperation.Addition, A, B, Output3);
            
        </member>
        <member name="T:BrainSimulator.Matrix.MyMatrixCublasOps">
            <summary>
            
            </summary>
        </member>
        <member name="M:BrainSimulator.Matrix.MyMatrixCublasOps.MatrixVectorMult(ManagedCuda.CudaDeviceVariable{System.Single},ManagedCuda.CudaDeviceVariable{System.Single},ManagedCuda.CudaDeviceVariable{System.Single},System.Int32,System.Int32,System.Boolean)">
            <summary>
              HONZA:   THIS IS FIXED, TRY RUN WITH CUDA CudaDeviceVariable AND IF IT WORKS, REPLACE THIS FUCNTION BY NEW ONE :D
            Computes y = Ax or y = A'x if <paramref name="transposeA"/> is true.
                ???? ADD IT INOT THE CODE !!!!!  ????
            </summary>
        </member>
        <member name="T:BrainSimulator.Matrix.MyMatrixKernelOps">
            <summary>
               Perform operations that are defined by kernel
            </summary>
        </member>
        <member name="T:BrainSimulator.Matrix.MatOperation">
            <summary>
            Operations that are allowed to run using the Matrix Node
            </summary>
        </member>
        <member name="T:BrainSimulator.Matrix.MyMatrixNode">
            <author> Honza Knopp</author>
            <status> Working </status>
               
            <summary>
              This performs several operations like addition, multiplication etc.
            </summary>
            <description>
            
            <h3> Features: </h3>
            <ul>
             <li> Allows multiplication addition with different input sizes, so instead of only A*B (where A,B are matrices), it supports A*v (v is vector), v*A or even const*A. </li>
             <li> For several opts (getRow, const*A), two input types are supported: 1) memory block from another node; 2) number in "ExecuteParams/DataInput".</li>
             <li> This is just a node layer above the MatrixAutoOps class, so you can use it simple in your code too.</li>
            </ul>
            
            <h3> Usage of operations: </h3>
            <ul>
             <li><b>Addition,Multiplication,Substraction,MultiplElemntWise </b> (two memBlock inputs, or one into A and set the DataInput0 paramter): Two inputs (each of them can be matrix, or vector, or constat). Be careful about the coorect sizes/dimensions of the inputs, it does column/row-wise operation. If only input to the A, then it perforsm multuiplication with the value at DataInput.</li>
             <li><b>DotProd </b> (two memBlock inputs): performs trans(vec) * vec. be carful about the size and dimensions.</li>
             <li><b>MinIndex, MaxIndex </b> (one mem block input): returns min/max index in the vector.</li>
             <li><b>GetCol,GetRow </b> (two memBlock inputs, or one into A and set the DataInput0 paramter): returns n-th column of the input. N can be DataInput or value in the memory block in the B-input.</li>
             <li><b>Minus </b> (one memBlock input): returns -A</li>
             <li><b>Normalize </b> (one memBlock input): return normalized matrix A, Norm2 used in this case.</li>
             <li><b>Norm2 </b> (one memBlock input): returns norm2 of the matrix A</li>
             <li><b>Exp, Abs, Log, Round, Floor, Ceil </b> (one memBlock input): returns Exp/Log/Abs/Floor/Round/Ceil of each element of the matrix A.</li>
            </ul>
            </description>
        </member>
        <member name="T:BrainSimulator.Matrix.MyMatrixNode.MyExecuteTask">
            <summary>
            paramter ,,Execute/params/DataInput0'' can be used for some operations when second input is not given. For example Addition, Multiplication, GetRow, GetCol...
            </summary>
        </member>
        <member name="T:BrainSimulator.Motor.MyJacobianTransposeControl">
            <author>Karol Kuna</author>
            <status>WIP</status>
            <summary>Computes set of torques that emulate effect of a virtual force applied to a point on body</summary>
            <description></description>
        </member>
        <member name="T:BrainSimulator.Motor.My3DPendulumWorld">
            <author>Karol Kuna</author>
            <status>WIP</status>
            <summary>3D World based on BEPUphysics engine.
            It can simulate various constraint systems defined via XML.</summary>
            <description></description>
        </member>
        <member name="T:BrainSimulator.Motor.My3DWorld">
            <author>Dusan Fedorcak</author>
            <status>WIP</status>
            <summary>3D World based on BEPUphysics engine.
            It can simulate various constraint systems defined via XML.</summary>
            <description></description>
        </member>
        <member name="T:BrainSimulator.Motor.My3DManipulatorWorld">
            <author>Dusan Fedorcak</author>
            <status>WIP</status>
            <summary>3D World based on BEPUphysics engine.
            It can simulate various constraint systems defined via XML.</summary>
            <description></description>
        </member>
        <member name="T:BrainSimulator.Motor.My3DBipedalRobotWorld">
            <author>Karol Kuna</author>
            <status>WIP</status>
            <summary>3D bipedal robot world based on BEPUphysics engine.</summary>
            <description></description>
        </member>
        <member name="T:BrainSimulator.Motor.MyArmWorld">
            <author>Karol Kuna</author>
            <status>Working</status>
            <summary>World simulating robotic arm</summary>
            <description></description>
        </member>
        <member name="T:BrainSimulator.Motor.MyColumnToVectorNode">
            <author>Karol Kuna</author>
            <status>Working</status>
            <summary>Transforms column from stators output in SEDroneWorld to vector</summary>
            <description></description>
        </member>
        <member name="T:BrainSimulator.Motor.MyControllerNode">
            <author>Karol Kuna</author>
            <status>Working</status>
            <summary>PID Controller</summary>
            <description></description>
        </member>
        <member name="T:BrainSimulator.Motor.MyMusclesNode">
            <author>Karol Kuna</author>
            <status>Working</status>
            <summary>Transforms binary alpha neuron activation into final motor output and backwards</summary>
            <description></description>
        </member>
        <member name="T:BrainSimulator.Motor.MyRecurrentNetwork">
            <author>Karol Kuna</author>
            <status>Working</status>
            <summary>Recurrent network with Elman architecture and Real-Time Recurrent Learning</summary>
            <description></description>
        </member>
        <member name="T:BrainSimulator.Motor.MySequenceRecorderNode">
            <author>Karol Kuna</author>
            <status>Working</status>
            <summary>Records recent input vectors into matrix (one row per time step) and plays the matrix back</summary>
            <description></description>
        </member>
        <member name="T:BrainSimulator.NeuralGas.MyGrowingNeuralGasNode">
            <author>Radoslav Bielek</author>
            <status>Working</status>
            <summary>Growing neural gas implementation with various growing mechanisms</summary>
            <description>
            Parameters:<br />
            <ul>
            <li>MAX_CELLS: maximum possible number of active neural gas cells</li>
            <li>INIT_LIVE_CELLS: the initiali active cell of neural gas</li>
            <li>INPUT_SIZE: size of the input vector, also the length of neural gas vector, set automatically, read only</li>
            </ul>
            </description>
        </member>
        <member name="T:BrainSimulator.NeuralGas.MyInitGrowingNeuralGasNodeTask">
            <summary>
            Initialization of the growing neural gas node
            Parameters:
            <ul>
            <li>MIN_INIT_INPUT: minimal value of the input and reference vector cell</li>
            <li>MAX_INIT_INPUT: maximal value of the input and reference vector cell</li>
            </ul>
            </summary>
        </member>
        <member name="T:BrainSimulator.NeuralGas.MyFindWinnersTask">
            <summary>
            Find the neural gas cell with best matching reference vector to the input
            </summary>
        </member>
        <member name="T:BrainSimulator.NeuralGas.MyAdaptWinningFractionTask">
            <summary>
            For GNG with consciousness modification it is needed to store the winning fraction of each neuron cell.<br />
            B_PARAM: learning rate of the winning fraction counter (the counter is based on the Hebbian learning rule)
            </summary>
        </member>
        <member name="T:BrainSimulator.NeuralGas.MyBiasTermTask">
            <summary>
            For GNG with consciousness modification this task compute biased winning neuron cell
            C_FACTOR: the strength of consciousness, if set to zero the algorithm is standard GNG
            </summary>
        </member>
        <member name="T:BrainSimulator.NeuralGas.MyFindConsciousWinnersTask">
            <summary>
            Find the winner with considering the consciousness bias
            </summary>
        </member>
        <member name="T:BrainSimulator.NeuralGas.MySendDataToOutputTask">
            <summary>
            Send all the neccessary data to the corresponding output fields
            </summary>
        </member>
        <member name="T:BrainSimulator.NeuralGas.MyAddLocalErrorAndUtilityTask">
            <summary>
            For the winning neural cell cummulate its error and the utility value
            </summary>
        </member>
        <member name="T:BrainSimulator.NeuralGas.MyCreateConnecionTask">
            <summary>
            Create connection between the winner 1 and 2 (if there isn't one already)
            </summary>
        </member>
        <member name="T:BrainSimulator.NeuralGas.MyAdaptRefVectorTask">
            <summary>
            Adapt the reference vector of the winner and its connected neural cell<br/>
            The adaptation can be set in a way that not trained cells (distinquished by the low winning count) are adapting faster than trained ones.
            Parameters:
            <ul>
            <li>E_b_YOUNG: the learning rate for the untrained winner cell</li>
            <li>E_b_OLD: the learning rate for the trained winner cell</li>
            <li>E_n_YOUNG: the learning rate for the untrained neighboring cell</li>
            <li>E_n_OLD: the learning rate for the trained neighboring cell</li>
            <li>DECAY_FACTOR: the decay factor of the learning rate, the learning rate changes exponentialy from _YOUNG to _OLD value</li>
            </ul>
            </summary>
        </member>
        <member name="T:BrainSimulator.NeuralGas.MyIncrementConnectionAgeTask">
            <summary>
            Increment the connection age emanating from the winner neuron cell
            </summary>
        </member>
        <member name="T:BrainSimulator.NeuralGas.MyRemoveConnsAndCellsTask">
            <summary>
            Remove the connections if they are old and cells if they don't have any connections or if they are not utilized<br/>
            Note: the neural cell removing according to utility factor was not really efficient and hard to set
            Parameters:<br/>
            <ul>
            <li>MAX_AGE: maximum possible age for the connection, if it is higher the connection is removed</li>
            <li>USE_UTILITY: if True - use the removing based on cell utility, if False - don't use it</li>
            <li>UTILITY: the utility threshold for removing the cells</li>
            </ul>
            </summary>
        </member>
        <member name="T:BrainSimulator.NeuralGas.MyAddNewNodeTask">
            <summary>
            Adding new nodes (neural cells) to the neural gas<br/>
            Rules:<br/>
            <ul>
            <li>INCREMENTAL: the new node is added each fixed number of steps in the place between node with the highest accumulated error and its neighbor with highest accumulated error</li>
            <li>VECTOR_OUTLIER: if the input is too far from the closest cell the new one is created with the reference vector same as the input</li>
            <li>EQUILIBRIUM: the new node is added every time when the average accumulated error reach a pre-defined value</li>
            </ul>
            Incremental:<br/>
            <ul>
            <li>LAMBDA - number of time steps when the new cell is added</li>
            <li>ALFA - fraction of the error which is subtracted from the neighbors of newly created neural cell</li>
            </ul>
            Vector outliers:<br/>
            <ul>
            <li>DISTANCE - the distance percentage between winner 1 and 2 which, if reached new neural cell is created on the place of input vector</li>
            </ul>
            Equilibrium:<br/>
            <ul>
            <li>AVG_E_TH - average error threshold, if reached new cell is created on the place as in Incremental adding rule</li>
            </ul>
            </summary>
        </member>
        <member name="T:BrainSimulator.NeuralGas.MyDecreaseErrorAndUtilityTask">
            <summary>
            Decrease accumulated error of each cell by the factor BETA
            </summary>
        </member>
        <member name="T:BrainSimulator.Retina.MyFocuser">
            <author>Dusan Fedorcak</author>
            <status>Working</status>
            <summary>Crops and resizes input image according to pupil control input.
            Pupil control input must contain position and size of focused area.</summary>
            <description></description>
        </member>
        <member name="T:BrainSimulator.Retina.MyPupilControl">
            <author>Dusan Fedorcak</author>
            <status>Working (K-Means only)</status>
            <summary>Analyzes visual input through growing K-Means clustering.
            Clusters are iterated and current cluster data (position and scale) is sent to the output.</summary>
            <description></description>
        </member>
        <member name="T:BrainSimulator.TicTacToe.MyTicTacToeWorld">
            <author>Jaroslav Vitku</author>
            <status>Under Development</status>
            <summary>Simulator of Tic Tac Toe game.</summary>
            <description>
            
            Player (the brain) has X and computer (world) has O. 
            Games are repeated. If the player wins, reward is received. If the computer wins, punishment is received.
            
            <h3>Outputs:</h3>
            <ul>
                <li> <b>Global:</b>Vectort of all state variables of the world  = [State, Reward, Punishment]</li>
                <li> <b>State:</b>Vector of length 9 with values {0,1,2} = {empty, player, computer}.</li>
                <li> <b>RewardEvent:</b> Reward is received if the player wins the game.</li>
                <li> <b>PunishmentEvent:</b> Punishment is received if the player wins the game.</li>
                <li> <b>Visual:</b> bitmap representing the current world state.</li>
            </ul>
            
            <h3>Inputs</h3>
            <ul>
                <li> <b>Action:</b> Vector of size 9 which chosen action. Action = place, where to put the X. If X is already taken, the step is missed and the player has another attempt.</li>
            </ul>
            
            </description>
        </member>
        <member name="T:BrainSimulator.TicTacToe.MyTicTacToeWorld.MyInitTask">
            <summary>
            Initialize the world (load graphics etc.).
            </summary>
        </member>
        <member name="T:BrainSimulator.TicTacToe.MyTicTacToeWorld.MyRenderTask">
            <summary>
            Renders the visible area, not needed for simulation.
            </summary>
        </member>
        <member name="T:BrainSimulator.TicTacToe.MyTicTacToeWorld.MyUpdateTask">
            <summary>
            Read action, if valid check for win, choose the computer action, check for win, 
            reset and publish reward/punishment if the game ends, publish new state.
            
            <description>
            <h3>Parameters</h3>
            <ul>
                <li><b>Difficulty: </b>How well the computer plays</li>
            </ul>
            </description>
            </summary>
        </member>
        <member name="T:CustomModels.TicTacToeWorld.ITicTacToeEngine">
            <summary>
            Thing, that plays the tic tac toe
            </summary>
        </member>
        <member name="M:BrainSimulator.Versioning.MyConversion.Convert1To2(System.String)">
            <summary>
            Convert RandomMapper task name and property names
            Author: Martin Milota
            </summary>
        </member>
        <member name="T:BrainSimulator.VSA.Hashes.MyHashingMemory">
            <author>Martin Milota</author>
            <status>WIP</status>
            <summary></summary>
            <description></description>
        </member>
        <member name="T:BrainSimulator.VSA.Hashes.MyHashMapper">
            <author>Martin Milota</author>
            <status>WIP</status>
            <summary>Constructs an index key for every element of the input. The range of the indices is [0, <seealso cref="P:BrainSimulator.VSA.Hashes.MyHashMapper.MyHashMapperTask.InternalBinCount"/>).
             You can optionally change the dimensionality of the input.</summary>
            <description></description>
        </member>
        <member name="M:BrainSimulator.VSA.MyRandomPool.GenerateRandomNormalVectors(System.Single[],System.Random,System.Int32,System.Int32,System.Single,System.Double,System.Boolean,System.Random,System.Single)">
            <summary>
            Fills the <paramref name="codeVectors"/> array with <paramref name="otherDim"/> random normally distributed vectors with size <paramref name="leadingDim"/>, zero mean and variance 1/<paramref name="var"/>.
            Each vector is normalized to unit length. Use the <paramref name="oneToZeroRatio"/> parameter to specify the sparseness of the vectors.
            <paramref name="otherDim"/> shall be the leading dimension of the resulting matrix.
            </summary>
            <param name="codeVectors">The array to populate by the random normal vectors.</param>
            <param name="random">The random object used to generate vector elements.</param>
            <param name="leadingDim">The size of each vector.</param>
            <param name="otherDim">The number of vectors to be generated.</param>
            <param name="mean">The expected mean of the generated values. Defaults to zero.</param>
            <param name="var">The variance of the generated values. If not specified or non-positive variance is passed, the value 1/xDim will be used to generate approximately normalized vectors.</param>
            <param name="normalize">Specifies whether each generated vector should be normalized to unit length.</param>
            <param name="oneToZeroRandom">The random object used to decide the sparseness of the vectors. If null is passed, <paramref name="oneToZeroRatio"/> is set to 1.</param>
            <param name="oneToZeroRatio">The ratio of non-zero elements to zeros.</param>
        </member>
        <member name="M:BrainSimulator.VSA.MyRandomPool.GenerateRandomBSCVectors(System.Single[],System.Random,System.Single)">
            <summary>
            Fills the <paramref name="codeVectors"/> array with random binary numbers.
            Use the <paramref name="oneToZeroRatio"/> parameter to specify the sparseness of the vectors.
            </summary>
            <param name="codeVectors">The array to populate by the random binary vectors.</param>
            <param name="random">The random object used to decide the sparseness of the vectors. If null is passed, <paramref name="oneToZeroRatio"/> is set to 1.</param>
            <param name="oneToZeroRatio">The ratio of non-zero elements to zeros.</param>
        </member>
        <member name="M:BrainSimulator.VSA.MyRandomPool.Transpose(System.Single[]@,System.Int32,System.Int32)">
            <summary>
            Transposes the matrix with dimensions specified by <paramref name="xDim"/> and <paramref name="yDim"/>.
            </summary>
        </member>
        <member name="M:BrainSimulator.VSA.MyRandomPool.NormalizeLeadingDim(System.Single[],System.Int32,System.Int32)">
            <summary>
            Normalizes vectors along the leading dimension.
            </summary>
        </member>
        <member name="M:BrainSimulator.VSA.MyRandomPool.NormalizeLeadingDim(BrainSimulator.Memory.MyMemoryBlock{System.Single},BrainSimulator.Memory.MyMemoryBlock{System.Single},System.Int32,System.Int32,BrainSimulator.MyCudaKernel,BrainSimulator.MyCudaKernel,System.Int32)">
            <summary>
            Normalizes vectors along the leading dimension.
            </summary>
        </member>
        <member name="M:BrainSimulator.VSA.MyRandomPool.GenerateTransformMatrix(System.Random,System.Int32,System.Int32,System.Boolean,BrainSimulator.VSA.MyRandomPool.AxisToNormalizeEnum)">
            <summary>
            Generates a matrix with <paramref name="yDim"/> being the leading dimension in column-major storage.
            </summary>
            <param name="random">The random object for number generation.</param>
            <param name="xDim">The size of the leading dimension.</param>
            <param name="yDim">The size of the other dimension.</param>
            <param name="orthonormalize">If true, the vectors along the longer dimension will be orthonormalized.</param>
            <param name="axisToNormalize">The axis along which to normalize vectors after orthonormalization.</param>
            <returns>The generated matrix.</returns>
        </member>
        <member name="M:BrainSimulator.VSA.MyRandomPool.GenerateTransformMatrix(BrainSimulator.Memory.MyMemoryBlock{System.Single},BrainSimulator.Memory.MyMemoryBlock{System.Single},BrainSimulator.Memory.MyMemoryBlock{System.Single},System.Random,System.Int32,System.Int32,BrainSimulator.MyCudaKernel,BrainSimulator.MyCudaKernel,BrainSimulator.MyCudaKernel,System.Int32,BrainSimulator.VSA.MyRandomPool.VectorGenerationMode,BrainSimulator.VSA.MyRandomPool.AxisToNormalizeEnum)">
            <summary>
            Generates a matrix with <paramref name="xDim"/> being the leading dimension in column-major storage.
            </summary>
            <param name="unmanagedVectors">A memory block to store the generated matrix.
            Must be as large as <paramref name="xDim"/> x <paramref name="yDim"/>.</param>
            <param name="unmanagedBaseVectors">A temporary block to store all the base vectors.
            Must be as large as Max(<paramref name="xDim"/>, <paramref name="yDim"/>)^2.
            Only neccessary when <paramref name="mode"/> is set to <see cref="F:BrainSimulator.VSA.MyRandomPool.VectorGenerationMode.AverageBaseVectors"/>.</param>
            <param name="temp">The temporary storage. It should be as long as the longer of the dimensions.</param>
            <param name="random">The random object for number generation.</param>
            <param name="xDim">The size of the other dimension.</param>
            <param name="yDim">The size of the leading dimension.</param>
            <param name="mode">If true, the vectors along the longer dimension will be orthonormalized.</param>
            <param name="axisToNormalize">The axis along which to normalize vectors after orthonormalization.</param>
        </member>
        <member name="M:BrainSimulator.VSA.MyRandomPool.DotProduct(System.ArraySegment{System.Single},System.ArraySegment{System.Single})">
            <summary>
            Computes the inner product of two vectors specified by the array segments.
            </summary>
            <param name="first">The first vector of the inner product.</param>
            <param name="second">The second vector of the inner product.</param>
            <returns>The inner products of the given vectors.</returns>
        </member>
        <member name="M:BrainSimulator.VSA.MyRandomPool.OrthonormalizeVectors(System.Single[],System.Int32,System.Int32)">
            <summary>
            Transforms all the vectors stored in <paramref name="vectors"/> to be pair-wise orthonormal using a modified version of the Gram-Schmidt algorithm.
            </summary>
            <param name="vectors">The vectors to orthonormalize.</param>
            <param name="xDim">The length of each vector.</param>
            <param name="yDim">The number of vectors.</param>
        </member>
        <member name="M:BrainSimulator.VSA.MyRandomPool.OrthonormalizeVectors(BrainSimulator.Memory.MyMemoryBlock{System.Single},BrainSimulator.Memory.MyMemoryBlock{System.Single},System.Int32,System.Int32,BrainSimulator.MyCudaKernel,BrainSimulator.MyCudaKernel,System.Int32)">
            <summary>
            Transforms all the vectors stored in <paramref name="vectors"/> to be pair-wise orthonormal using a modified version of the Gram-Schmidt algorithm.
            </summary>
            <param name="vectors">The vectors to orthonormalize.</param>
            <param name="temp">A vector of temporal space.</param>
            <param name="xDim">The length of each vector.</param>
            <param name="yDim">The number of vectors.</param>
            <param name="dotKernel">The kernel to compute a dot product.</param>
            <param name="multKernel">The kernel to compute vector combinations.</param>
        </member>
        <member name="M:BrainSimulator.VSA.Hashes.MyHashMapper.MyHashMapperTask.GetIndices(ManagedCuda.BasicTypes.CUdeviceptr,ManagedCuda.BasicTypes.CUdeviceptr,ManagedCuda.BasicTypes.CUdeviceptr,System.Nullable{ManagedCuda.BasicTypes.CUdeviceptr},System.Int32,System.Int32,System.Int32,BrainSimulator.MyCudaKernel,BrainSimulator.MyCudaKernel,BrainSimulator.MyCudaKernel,System.Boolean,System.Int32,System.Boolean)">
            <summary>
            Transforms the <paramref name="output"/> vector into a vector of indices with properties specified by the parameters.
            </summary>
            <param name="input">The vector to transform.</param>
            <param name="output">The memory to contain the results.</param>
            <param name="misc">A vector containing the range to modulate to as the first value (typically 2f because dot product ranges from [-1,1])
            and the bin size in this modulated space (typically <paramref name="misc"/>[0] / internalBinCount) as the second value.</param>
            <param name="offsets">The random offsets for each <paramref name="output"/> value (typically uniform random numbers in [0, <paramref name="misc"/>[0].</param>
            <param name="vectorSize">The length of the <paramref name="output"/> vector.</param>
            <param name="outputBinCount">The range into which the internal bins will be scattered.</param>
            <param name="seed">The seed used for the scattering the internal bins.</param>
            <param name="combineVectorsKernel">The kernel used for addition, modulo and integer division.</param>
            <param name="hashKernel">The kernel used for scattering the internal bins.</param>
            <param name="doScattering">If true, each internal bin will be randomly scattered to an integer in [0, <paramref name="outputBinCount"/>).
            Otherwise, the range of the output indices will be [0, internalBinCount)</param>
        </member>
        <member name="T:BrainSimulator.Vision.MyJoinPatches">
            <author>Honza Knopp</author>
            <status> Working / Unoptimized as propagation part is in C# instead of cuda!</status>
            <summary>
              Concatenate patches/segments into objects.
            </summary>
            <description>
            Takes as input set of patches/superpixels (each defined with its location x,y) and joins them into a groups,
            in each group there has to exists a path from one input segment (S1) to the another one (S2) such that
            distance between each ngh. nodes on one way between S1 and S2 is smaller then the treshold.
            
              <h4> Inputs:</h4>
               <ul>
                <li>Patches: [nx3]   matrix of x,y,scale of N patches/superpixels/objects.</li>
                <li>Desc:    [nxdim] descriptor of each path/superpixel/objects.</li>
                <li>Mask:    [axb]   image where each pixels value corresponds to the segmetn id of the pixel.</li>
               </ul>
               
              <h4> Outputs:</h4>
               Same like input, but conncatenated. In Patches and Desc, there is -1 where it is empty.
               
              <h4>Observer:</h4>
               When observer is visualizing the result, you can press a key to change what is shows:
              <ul>
               <li> default: new mask </li>
               <li> 6:       graph of neigh. segmetns</li>
               <li> 7:       graph with weights that is used for propagation that results in connected compenents </li>
              </ul>
             
            </description>
        </member>
        <member name="P:BrainSimulator.Vision.MyJoinPatches.AdjMatrix">
            dimension of NEW Optical flow descriptor
        </member>
        <member name="T:BrainSimulator.Vision.MyJoinPatches.MyProcessImPatchBasTask">
            <summary>
            Execute joining patches into groups of objects.
            </summary>
        </member>
        <member name="T:BrainSimulator.Vision.MyKMeansWM">
            <author> Honza and Michal</author>
            <status> Working</status>
            <summary>
               On-the-fly clustering of incoming data. The node expects descriptor of the patch and its position. Then. it assignes cluster id to the input and change the clusters.
            </summary>
            <description>
               Sometimes it sets NearestCC_id to -1. Check it!
            <h3>Paramters</h3>
             <ul> 
               <li> MaxClusters: Maximum number of elements in the memory. </li>
               <li> ThresholdDescSim: Inverse distance that is necasaty to assign input data into the existing memory element.</li>
               <li> UpdateClusterCentersOrdering: Will I upodate database?</li>
               <li> WeightXY: how to prefer XY similarity vs. descritpro similarity.</li>
              </ul>
            </description>
        </member>
        <member name="T:BrainSimulator.Vision.MyKMeansWM.MyKMeansWMExecuteTask">
            <summary>
             Process input data
            </summary>
        </member>
        <!-- Badly formed XML comment ignored for member "M:BrainSimulator.Vision.MyKMeansWM.MyKMeansWMExecuteTask.NormalizeVector(BrainSimulator.Memory.MyMemoryBlock{System.Single},System.Int32,System.Int32)" -->
        <member name="T:BrainSimulator.Vision.MyKMeansWM.MyKMeansWMReSortOutputTask">
            <summary>
            In each iteration step, resort databse that most occuring elements will be at the top of the memory block.
            </summary>
        </member>
        <member name="M:BrainSimulator.Vision.MyKMeansWM.MyKMeansWMReSortOutputTask.Reshuffle_Mat(BrainSimulator.Memory.MyMemoryBlock{System.Single},BrainSimulator.Memory.MyMemoryBlock{System.Single},BrainSimulator.Memory.MyMemoryBlock{System.Single})">
            <summary>
              Change the ordering of vectors in Mat-matrix based on the indexes in IdxOrdering
            </summary>
        </member>
        <member name="T:BrainSimulator.Vision.MySaccade">
            <author>Honza Knopp</author>
            <status>Working </status>
            <summary>
              Select patch/segment/object for Saccadic movement...
            </summary>
            <description>
              <h4> Input </h4>
              <ul>
                <li>Patches: [nx3]   matrix of x,y,scale of N patches/superpixels/objects. (-1 are ignored)</li>
                <li>Desc:    [nxdim] descriptor of each path/superpixel/objects.</li>
                <li>Mask:    [axb]   image where each pixels value corresponds to the segmetn id of the pixel.</li>
              </ul> 
            </description>
        </member>
        <member name="P:BrainSimulator.Vision.MySaccade.DescNum">
            real number of ppatches as there can be some zeros... :)
        </member>
        <member name="T:BrainSimulator.Vision.MySaccade.MySaccadeInitTask">
            <summary>
            Set default values for enery terms. It is done only once.
            </summary>
        </member>
        <member name="T:BrainSimulator.Vision.MySaccade.MySaccadeUpdateTask">
            <summary>
            Update terms based on the actual inputs -> now we have id of min energy.
              <h4> Paramters </h4>
              <ul>
                <li> IncreaseOnFocus:       Term increase when focuser selects it. Lower will stay on the object longer</li>
                <li> DecreaseInTime:        Ration for decresing time term in time. Higher will try to go back on the seen patch sooner.</li>
                <li> RationSupportMovement: How much to prefer movement against time.</li>
                <li> RationSupportBrainFavour: how to favour brains need where to focus.</li>
            </ul>
            </summary>
        </member>
        <member name="M:BrainSimulator.Vision.MySaccade.MySaccadeUpdateTask.EstimateRealNumOfPatches">
            <summary>
            Find where zeros start -> this estimates # of patches...
            </summary>
            <returns></returns>
        </member>
        <member name="T:BrainSimulator.Vision.MySaccade.MySaccadeConvertOutId2XYTask">
            <summary>
            Convert id into a xy+scale and set it as output :)
            </summary>
        </member>
        <member name="T:BrainSimulator.Vision.MySegment">
            <author>Honza Knopp</author>
            <status>Working, while restriceted to square images and specific nSegments values</status>
            <summary>
              Segment image into a set of superpixels.
            </summary>
            <description>
              Takes image (RGB or just Black and White if G/B input channels are left free) as an input and runs SLIC algorithm to find segments.
              
              <h4> Input</h4>
                Image. When it is gray, use only the first branch.
              
              <h4> Output</h4>
              <ul>
                <li> SP_xy:    Center of each superpixel (segment) [x,y,nPts]  (need #of pts). It has size nSeg x 3.</li>
                <li> SP_desc:  Descriptor [r,g,b,movement] it has size nSeg x 4.</li>
                <li> Mask:     Mask where the value of each pixel has segmentId. it has size of the input image.</li>
              </ul>
              
              <h4> Parameters </h4>
              <ul>
                <li> nSegs:  Number of segmetns. Try to keep number such that int n exist: nSegs=n*n; Higher->faster.</li>
                <li> Weight: Whether segmetns should prefer grid structure. 0.9 works the best for fishes, while 0.3 for phong.</li>
                <li> nIters: Number of iterations. Something between 1-3 is usally more than enough.</li>
              </ul>
              
              <h4>Restrictions</h4>
              <ul>
                <li> The width of the image has to correspond to its height.</li>
                <li> If n is number of segments, there has to exist integer k such that k*k=n.</li>
                <li> It is better to keep number of segments per rows/columns nicely divided in the context of number of pixels per row/column.</li>
              </ul>
              
              <h4>Observer</h4>
               When observer is visualizing the result, you can press a key to change what is shows:
              <ul>
                <li> default: borders of segments</li>
                <li> 1: XYZ space of colors</li>
                <li> 2: id of segments </li>
                <li> 3: id os segments normalized for beter view </li>
                <li> 4: center of each segment</li>
             </ul>
            </description>
        </member>
        <member name="P:BrainSimulator.Vision.MySegment.floatBuffer">
            # of segemtns
        </member>
        <member name="F:BrainSimulator.Vision.MySegment.MySLICClusterCenterObject.dummy">
            Why the hell does he use/define these paramters?
        </member>
        <member name="T:BrainSimulator.Vision.MySegment.MySLICTask">
            <summary>
            Run SLIC algorithm that calculates segments.
            </summary>
        </member>
        <member name="F:BrainSimulator.Vision.MySegment.MySLICTask.m_kernel_init">
            # of iterations
        </member>
        <member name="F:BrainSimulator.Vision.MySegment.MySLICTask.nWidth">
            this needs to correpond to the same var in the kernel!
        </member>
        <member name="F:BrainSimulator.Vision.MySegment.MySLICTask.nHeight">
            this needs to correpond to the same var in the kernel!
        </member>
        <member name="F:BrainSimulator.Vision.MySegment.MySLICTask.nClustersPerRow">
            this needs to correpond to the same var in the kernel!
        </member>
        <member name="F:BrainSimulator.Vision.MySegment.MySLICTask.nClustersPerCol">
            this needs to correpond to the same var in the kernel!
        </member>
        <member name="P:BrainSimulator.Vision.MySegment.MySLICTask.nIters">
            How to prefer grid result
        </member>
        <member name="T:BrainSimulator.Vision.MySegment.MyCalcDescTask">
            <summary>
            Calculate desriptor - it has color desc and movement (differnece to previous result). Also commented code for soemthing with edges, but it is useless and should be done in a better way.
            </summary>
        </member>
        <member name="T:BrainSimulator.VSA.MyCombinationBook">
            <author>Good AI</author>
            <tag>#mm</tag>
            <status>Not Optimized</status>
            <summary>Generates static random permutations with either a single cycle or random cycles.
             Optionally, generates random combinations with either unique numbers or allowed duplicates.</summary>
            <description>A single cycle is created by applying the Sattolo's shuffle (as seen on <seealso cref="!:http://en.wikipedia.org/wiki/Fisher–Yates_shuffle#Sattolo.27s_algorithm"/>). 
             Random cycles are created by using the Fisher-Yates shuffle (as seen on <seealso cref="!:http://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle#The_.22inside-out.22_algorithm"/>).
             Use the <see cref="P:BrainSimulator.VSA.MyCombinationBook.Power"/> property to pre-compute a desired power of the base permutation.</description>
        </member>
        <member name="M:BrainSimulator.VSA.MyCombinationBase.ShuffleSattolo(System.ArraySegment{System.Single},System.Random,System.Boolean)">
            <summary>
            Shuffles a part of a sequence. The permutation in the segment has only one cycle (there are (n-1)! such permutations).
            For more information see http://en.wikipedia.org/wiki/Fisher–Yates_shuffle#Sattolo.27s_algorithm
            </summary>
            <param name="seq">The segment to be shuffled.</param>
            <param name="rnd">The random number generator used to permute the sequence.</param>
            <param name="inPlaceInit">If true, will generate a permutation of numbers from (0, seq.Count - 1). Otherwise, it will permute the original sequence.</param>
            <returns>The random permutation.</returns>
        </member>
        <member name="M:BrainSimulator.VSA.MyCombinationBase.ShuffleFisherYates(System.ArraySegment{System.Single},System.Random,System.Boolean)">
            <summary>
            Shuffles a part of a sequence. The permutation in the segment has random cycles (there are n! such permutations).
            For more information see http://en.wikipedia.org/wiki/Fisher%E2%80%93Yates_shuffle#The_.22inside-out.22_algorithm
            </summary>
            <param name="seq">The segment to be shuffled.</param>
            <param name="rnd">The random number generator used to permute the sequence.</param>
            <param name="inPlaceInit">If true, will generate a permutation of numbers from (0, seq.Count - 1). Otherwise, it will permute the original sequence.</param>
            <returns>The random permutation.</returns>
        </member>
        <member name="M:BrainSimulator.VSA.MyCombinationBase.GenerateCombinationUnique(System.ArraySegment{System.Single},System.Collections.Generic.HashSet{System.Single},System.Int32,System.Int32,System.Random)">
            <summary>
            Generates unique random numbers in the range [<paramref name="min"/>, <paramref name="max"/>), where min is inclusive and max is exclusive and stores them in <paramref name="arr"/>.
            For more information see http://codereview.stackexchange.com/a/61372
            </summary>
            <param name="arr">The array to store the resulting combinations in. The whole array will be populated.</param>
            <param name="candidates">The temporary hash table to store intermediate results in.</param>
            <param name="min">The minimum value of a resulting element.</param>
            <param name="max">The maximum value of a resulting element.</param>
            <param name="rnd">The random number generator used to permute the sequence.</param>
            <returns>The random sample.</returns>
        </member>
        <member name="M:BrainSimulator.VSA.MyCombinationBase.GenerateCombination(System.ArraySegment{System.Single},System.Int32,System.Int32,System.Random)">
            <summary>
            Generates non-unique random numbers in the range [<paramref name="min"/>, <paramref name="max"/>), where min is inclusive and max is exclusive and stores them in <paramref name="arr"/>.
            </summary>
            <param name="arr">The array to store the resulting combinations in. The whole array will be populated.</param>
            <param name="min">The minimum value of a resulting element.</param>
            <param name="max">The maximum value of a resulting element.</param>
            <param name="rnd">The random number generator used to permute the sequence.</param>
            <returns>The random sample.</returns>
        </member>
        <member name="M:BrainSimulator.VSA.MyCombinationBook.GetPowerString(System.Int32)">
            <summary>
            Returns a superscript string representing the parameter.
            </summary>
        </member>
        <member name="M:BrainSimulator.VSA.MyCombinationBook.MakePower(ManagedCuda.BasicTypes.CUdeviceptr,ManagedCuda.BasicTypes.CUdeviceptr,ManagedCuda.CudaDeviceVariable{System.Single},BrainSimulator.MyCudaKernel,System.Int32,System.Int32)">
            <summary>
            Applies the <paramref name="multKernel"/> operation on <paramref name="identity"/> and <paramref name="codeVector"/> <paramref name="power"/>-times.
            </summary>
        </member>
        <member name="T:BrainSimulator.VSA.MyCodeBook">
            <author>Good AI</author>
            <tag>#mm</tag>
            <status>Working</status>
            <summary>Static symbol table shared between all nodes of the same <see cref="P:BrainSimulator.VSA.MyRandomPool.SymbolSize"/>.
            Use <see cref="P:BrainSimulator.VSA.MyCodeBook.Mode"/> to specify whether to generate a symbol or test it through dot product with the input.</summary>
            <description></description>
        </member>
        <member name="T:BrainSimulator.VSA.MyBindingNode">
            <author>Good AI</author> 
            <tag>#mm</tag>
            <status>Working</status>
            <summary>
            BindInternal and unbind operations for symbols.
            Based on fast Fourier transformation.
            </summary>
        </member>
        <member name="T:BrainSimulator.VSA.MyRandomMapper">
            <author>Good AI</author>
            <tag>#mm</tag>
            <status>Working</status>
            <summary>Performs a random projection to another dimension by computing A.x with a random matrix A and the input vector x.
            The transformation matrix is shared with nodes that do the same transformation (including its inverse, see <see cref="P:BrainSimulator.VSA.MyRandomMapper.DoDecoding"/>)
            and have the same <see cref="P:BrainSimulator.VSA.MyRandomMapper.NameGroup"/>. You can select which matrix axis to normalize to produce vectors of different lengths.</summary>
            <description></description>
        </member>
        <member name="T:BrainSimulator.VSA.MySpatialCoder">
            <author>Dusan Fedorcak</author>
            <status>WIP</status>
            <summary>Encodes &amp; decodes spatial values into symbols through linear interpolation</summary>
            <description></description>
        </member>
        <member name="T:BrainSimulator.VSA.MySymbolicGrid">
            <author>Dusan Fedorcak, Radoslav Bielek</author>
            <status>Working, binary symbols not implemented</status>
            <summary>Symbolic representataion of uniform square grid.
            Will be used for storing spatial data into symbols.</summary>
            <description>
            Parameters:<br />
            <ul>
            <li>X_POINTS - number of points (symbols) on the X-axis</li>
            <li>Y_POINTS - number of points (symbols) on the Y-axis</li>
            <li>SYMBOL_COUNT - overall number of symbols (X_POINTS + Y_POINTS), read only</li>
            <li>Mode - mode of the symbolic grid mode: Encode - encode x and y to the symbol, Decode - decode symbol to the x and y positions, Cleanup - clean symbol on the input</li>
            </ul>
            </description>
        </member>
        <member name="T:BrainSimulator.VSA.MySymbolicGrid.MyInitGridTask">
            <summary>
            Initialization task for symbolic grid node<br />
            The space is discretized and symbols are generated
            </summary>
        </member>
        <member name="T:BrainSimulator.VSA.MySymbolicGrid.MySymbolizePositionTask">
            <summary>
            According to the Mode set in the node parameters, this tasks provides encoding, decoding or cleaning up the symbol<br />
            Parameters:<br />
            <ul>
            <li>X_MAX: maximum possible value on the x-axis (encodes interval  &lt;0,X_MAX &gt;)</li>
            <li>Y_MAX: maximum possible value on the y-axis (encodes interval  &lt;0,Y_MAX &gt;)</li>
            </ul>
            </summary>
        </member>
        <member name="T:BrainSimulator.VSA.MyTransformLearningNode">
            <author>Peter Hrosso</author>
            <status>Working</status>
            <summary>Learning transformations of Symbolic Pointers with gradient method</summary>
            <description>Works (probably) only on linear transformations</description>
        </member>
    </members>
</doc>
